{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14474298",
   "metadata": {},
   "source": [
    "\n",
    "# PRT565 â€” Assessment 2 (Machine Learning Coding Exercise)\n",
    "\n",
    "**Student Name:** Md Sajjad Hossain Sawran  \n",
    "**Student ID:** s377923  \n",
    "\n",
    "**Dataset:** `heart (1).csv` (Kaggle upload)\n",
    "\n",
    "**Aim:** Develop a ML-based model using **Logistic Regression**, **Decision Tree**, and **Random Forest** on the uploaded dataset.  \n",
    "This notebook is fully commented and runs end-to-end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df86ed56",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Instructions\n",
    "1. This notebook already points to your uploaded file.  \n",
    "2. If you change files, update `DATA_PATH` and (if needed) `TARGET_COL` below.  \n",
    "3. Run cells **top to bottom**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2402696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Imports & Configuration\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, RocCurveDisplay, classification_report\n",
    ")\n",
    "\n",
    "import joblib\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (7, 5)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "print('Ready. sklearn version:', __import__('sklearn').__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61911baa",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Data loading\n",
    "- `DATA_PATH` points to your Kaggle CSV.  \n",
    "- `TARGET_COL` is auto-detected if possible; adjust if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7312e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Configure dataset path and target column\n",
    "DATA_PATH = Path(r\"/mnt/data/heart (1).csv\")\n",
    "DETECTED_TARGET = 'target'\n",
    "TARGET_COL = DETECTED_TARGET if DETECTED_TARGET is not None else 'HeartDisease'\n",
    "\n",
    "print('Loading dataset from:', DATA_PATH)\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print('Shape:', df.shape)\n",
    "print('Columns:', list(df.columns))\n",
    "\n",
    "if TARGET_COL not in df.columns:\n",
    "    print(f\"WARNING: Target column '{TARGET_COL}' not found. Please set TARGET_COL to the correct name.\")\n",
    "    # Fallback search\n",
    "    for c in ['HeartDisease','target','Target','output','Output','Outcome','class','Class','label','Label']:\n",
    "        if c in df.columns:\n",
    "            TARGET_COL = c\n",
    "            print('Auto-switched TARGET_COL to:', TARGET_COL)\n",
    "            break\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c374d69",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Quick data audit\n",
    "- Dtypes, numeric summary, missing values, and class balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3b2f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\nDTypes:\\n', df.dtypes)\n",
    "print('\\nDescribe (numeric):\\n', df.describe(numeric_only=True).T)\n",
    "\n",
    "missing = df.isna().sum().sort_values(ascending=False)\n",
    "print('\\nMissing values per column:\\n', missing)\n",
    "\n",
    "print('\\nTarget value counts:')\n",
    "print(df[TARGET_COL].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f993eb7a",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Train / Test split\n",
    "- 25% test, stratified by target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52984a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "print('Train shape:', X_train.shape, ' Test shape:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9212eb",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Column typing\n",
    "- Auto-detect numeric vs categorical; adjust manually if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10e19a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeric_cols = X_train.select_dtypes(include=['int64','float64','int32','float32']).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
    "\n",
    "print('Numeric columns:', numeric_cols)\n",
    "print('Categorical columns:', categorical_cols)\n",
    "assert set(numeric_cols + categorical_cols) == set(X_train.columns), 'Column typing mismatch.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d0ca5b",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Preprocessing pipelines\n",
    "- Numeric: median imputer + standard scaler  \n",
    "- Categorical: most_frequent imputer + one-hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214ae3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeric_preprocess = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
    "])\n",
    "\n",
    "categorical_preprocess = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_preprocess, numeric_cols),\n",
    "    ('cat', categorical_preprocess, categorical_cols),\n",
    "])\n",
    "preprocess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b51e3c7",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Define models + hyperparameter grids\n",
    "- Logistic Regression, Decision Tree, Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083905d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models_and_grids = []\n",
    "\n",
    "# Logistic Regression\n",
    "logreg_pipe = Pipeline(steps=[\n",
    "    ('prep', preprocess),\n",
    "    ('clf', LogisticRegression(max_iter=2000, random_state=RANDOM_STATE, solver='liblinear'))\n",
    "])\n",
    "logreg_grid = {\n",
    "    'clf__C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'clf__penalty': ['l1', 'l2'],\n",
    "}\n",
    "models_and_grids.append(('LogisticRegression', logreg_pipe, logreg_grid))\n",
    "\n",
    "# Decision Tree\n",
    "dt_pipe = Pipeline(steps=[\n",
    "    ('prep', preprocess),\n",
    "    ('clf', DecisionTreeClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "dt_grid = {\n",
    "    'clf__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'clf__max_depth': [None, 4, 6, 8, 12, 16],\n",
    "    'clf__min_samples_split': [2, 5, 10],\n",
    "    'clf__min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "models_and_grids.append(('DecisionTree', dt_pipe, dt_grid))\n",
    "\n",
    "# Random Forest\n",
    "rf_pipe = Pipeline(steps=[\n",
    "    ('prep', preprocess),\n",
    "    ('clf', RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1))\n",
    "])\n",
    "rf_grid = {\n",
    "    'clf__n_estimators': [100, 300, 500],\n",
    "    'clf__max_depth': [None, 6, 10, 16],\n",
    "    'clf__min_samples_split': [2, 5, 10],\n",
    "    'clf__min_samples_leaf': [1, 2, 4],\n",
    "    'clf__max_features': ['sqrt', 'log2', None],\n",
    "}\n",
    "models_and_grids.append(('RandomForest', rf_pipe, rf_grid))\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "print('Pipelines and grids ready.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660eb1f2",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Hyperparameter tuning (GridSearchCV, 5-fold)\n",
    "Scoring: **accuracy**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd589a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search_results = {}\n",
    "for name, pipe, grid in models_and_grids:\n",
    "    print(f'\\n=== Tuning {name} ===')\n",
    "    gs = GridSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_grid=grid,\n",
    "        scoring='accuracy',\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        refit=True,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    gs.fit(X_train, y_train)\n",
    "    search_results[name] = gs\n",
    "    print(f'Best CV accuracy for {name}: {gs.best_score_:.4f}')\n",
    "    print('Best params:', gs.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aebc39",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Evaluation on the test set\n",
    "- Accuracy, Precision, Recall, F1, ROC-AUC  \n",
    "- Confusion Matrix & Classification Report  \n",
    "- ROC curves for all models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435fa8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(name, estimator, X_te, y_te):\n",
    "    y_pred = estimator.predict(X_te)\n",
    "    if hasattr(estimator, 'predict_proba'):\n",
    "        y_prob = estimator.predict_proba(X_te)[:, 1]\n",
    "    elif hasattr(estimator, 'decision_function'):\n",
    "        raw = estimator.decision_function(X_te)\n",
    "        y_prob = (raw - raw.min()) / (raw.max() - raw.min() + 1e-9)\n",
    "    else:\n",
    "        y_prob = np.zeros_like(y_pred, dtype=float)\n",
    "\n",
    "    acc = accuracy_score(y_te, y_pred)\n",
    "    prec = precision_score(y_te, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_te, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_te, y_pred, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_te, y_prob)\n",
    "    except ValueError:\n",
    "        auc = np.nan\n",
    "    cm = confusion_matrix(y_te, y_pred)\n",
    "    print(f'\\n[{name}] Test metrics')\n",
    "    print(f'Accuracy:  {{acc:.4f}}')\n",
    "    print(f'Precision: {{prec:.4f}}')\n",
    "    print(f'Recall:    {{rec:.4f}}')\n",
    "    print(f'F1-score:  {{f1:.4f}}')\n",
    "    print(f'ROC-AUC:   {{auc:.4f}}')\n",
    "    print('\\nConfusion Matrix:\\n', cm)\n",
    "    print('\\nClassification Report:\\n', classification_report(y_te, y_pred, zero_division=0))\n",
    "    return {{'name': name, 'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1, 'auc': auc, 'cm': cm}}\n",
    "\n",
    "test_summaries = []\n",
    "\n",
    "# ROC curves\n",
    "fig = plt.figure()\n",
    "for name, gs in search_results.items():\n",
    "    est = gs.best_estimator_\n",
    "    try:\n",
    "        RocCurveDisplay.from_estimator(est, X_test, y_test, name=name)\n",
    "    except Exception:\n",
    "        y_dummy = np.ones_like(y_test)\n",
    "        RocCurveDisplay.from_predictions(y_test, y_dummy, name=name)\n",
    "plt.title('ROC Curves (Test Set)')\n",
    "plt.show()\n",
    "\n",
    "for name, gs in search_results.items():\n",
    "    test_summaries.append(evaluate_model(name, gs.best_estimator_, X_test, y_test))\n",
    "\n",
    "summary_df = pd.DataFrame(test_summaries).sort_values(by='acc', ascending=False)\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5520327a",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Model interpretation\n",
    "- Logistic Regression coefficients  \n",
    "- Tree/Forest feature importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37633465",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_feature_names(preprocess, numeric_cols, categorical_cols):\n",
    "    ohe = preprocess.named_transformers_['cat'].named_steps['onehot']\n",
    "    ohe_names = ohe.get_feature_names_out(categorical_cols).tolist()\n",
    "    return numeric_cols + ohe_names\n",
    "\n",
    "for name, gs in search_results.items():\n",
    "    est = gs.best_estimator_\n",
    "    print(f'\\n=== {name}: Interpretation ===')\n",
    "    prep = est.named_steps['prep']\n",
    "    feat_names = get_feature_names(prep, numeric_cols, categorical_cols)\n",
    "\n",
    "    clf = est.named_steps['clf']\n",
    "    if isinstance(clf, LogisticRegression):\n",
    "        coefs = pd.Series(clf.coef_.ravel(), index=feat_names).sort_values()\n",
    "        print('Top negative coefficients (most protective):')\n",
    "        print(coefs.head(10))\n",
    "        print('\\nTop positive coefficients (most risky):')\n",
    "        print(coefs.tail(10))\n",
    "    elif isinstance(clf, (DecisionTreeClassifier, RandomForestClassifier)):\n",
    "        importances = pd.Series(clf.feature_importances_, index=feat_names).sort_values(ascending=False)\n",
    "        print('Top feature importances:')\n",
    "        print(importances.head(15))\n",
    "    else:\n",
    "        print('No built-in interpretation for this estimator.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f77b24",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Save artifacts\n",
    "- Best model by test accuracy  \n",
    "- Processed train/test splits  \n",
    "- Metrics summary CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8b2763",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_row = max(test_summaries, key=lambda d: d['acc'])\n",
    "best_name = best_row['name']\n",
    "best_model = search_results[best_name].best_estimator_\n",
    "\n",
    "out_dir = Path('outputs')\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model_path = out_dir / f'best_model_{best_name}.pkl'\n",
    "joblib.dump(best_model, model_path)\n",
    "\n",
    "X_train_assign = X_train.copy()\n",
    "X_test_assign = X_test.copy()\n",
    "X_train_assign[TARGET_COL] = y_train.values\n",
    "X_test_assign[TARGET_COL] = y_test.values\n",
    "X_train_assign.to_csv(out_dir / 'train_split.csv', index=False)\n",
    "X_test_assign.to_csv(out_dir / 'test_split.csv', index=False)\n",
    "\n",
    "summary_df.to_csv(out_dir / 'test_metrics_summary.csv', index=False)\n",
    "\n",
    "print(f'Saved best model to: {model_path.resolve()}')\n",
    "print('Artifacts in:', out_dir.resolve())\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cf65fd",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Submission reminder\n",
    "- Submit this **.ipynb** (with outputs) and your **CSV** file from Kaggle.  \n",
    "- Ensure metrics and plots are visible after running.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
